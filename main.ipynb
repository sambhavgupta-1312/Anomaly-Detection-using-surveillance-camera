{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/seymanurakti/fight-detection-surv-dataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fight_data_path = '/content/fight-detection-surv-dataset/fight'\n",
    "non_fight_data_path = '/content/fight-detection-surv-dataset/noFight'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### frame extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def extract_frames(video_path, num_frames):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Unable to open video file {video_path}\")\n",
    "        return frames\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    interval = total_frames // num_frames\n",
    "\n",
    "    for count in range(num_frames):\n",
    "        frame_number = count * interval\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"Error: Unable to read frame {frame_number}\")\n",
    "            continue\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "    # print(f\"Extracted {len(frames)} frames from {video_path}\")\n",
    "    return frames\n",
    "\n",
    "# Example usage\n",
    "# video_path = \"path_to_your_video.mp4\"\n",
    "# num_frames = 10\n",
    "\n",
    "# frames = extract_frames(video_path, num_frames)\n",
    "# print(f\"Total frames extracted: {len(frames)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames=extract_frames('/content/fight-detection-surv-dataset/fight/fi010.mp4',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fight_video_data_path = glob.iglob('*.mp4',root_dir=fight_data_path)\n",
    "non_fight_video_data_path = glob.iglob('*.mp4', root_dir=non_fight_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fight_frames=[]\n",
    "for i,path in enumerate(fight_video_data_path):\n",
    "    video_path=fight_data_path + '/' + path\n",
    "    fight_frames.append(extract_frames(video_path,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_fight_frames=[]\n",
    "for i,path in enumerate(non_fight_video_data_path):\n",
    "    video_path=non_fight_data_path + '/' + path\n",
    "    non_fight_frames.append(extract_frames(video_path,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture('/content/fight-detection-surv-dataset/fight/fi003.mp4')\n",
    "frame_number = 0\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "ret, frame = cap.read()\n",
    "print(frame.shape)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_frame(frame, target_size, interpolation=cv2.INTER_CUBIC):\n",
    "    return cv2.resize(frame, target_size, interpolation=interpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_video(list_of_frames, target_size):\n",
    "    resized_video = []\n",
    "    for frame in list_of_frames:\n",
    "      resized_frame=resize_frame(frame,target_size)\n",
    "      resized_video.append(resized_frame)\n",
    "    return resized_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fight_resize=[]\n",
    "target_size=(224,224)\n",
    "i=0\n",
    "for frame_list in fight_frames:\n",
    "  fight_resize.append(resize_video(frame_list,target_size))\n",
    "  # print('list {} resized'.format(i))\n",
    "  i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fight_resize[9][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_fight_resize=[]\n",
    "target_size=(224,224)\n",
    "i=0\n",
    "for frame_list in non_fight_frames:\n",
    "  non_fight_resize.append(resize_video(frame_list,target_size))\n",
    "  # print('list {} resized'.format(i))\n",
    "  i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_fight_resize[3][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to augment a single frame\n",
    "def augment_frame(frame,augmentation_pipeline):\n",
    "    augmented = augmentation_pipeline(image=frame)\n",
    "    return augmented['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_frames(frames_list, num_augmented_per_frame):\n",
    "    augmented_frames = []\n",
    "    augmentation_pipeline = A.Compose([\n",
    "        A.HorizontalFlip(p=0.4),                    # Apply horizontal flip with 50% probability\n",
    "        A.RandomBrightnessContrast(p=0.48),          # Adjust brightness and contrast with 30% probability\n",
    "        A.GaussianBlur(blur_limit=(1, 3), p=0.25),   # Apply Gaussian blur with 20% probability\n",
    "        A.RandomScale(scale_limit=(-0.15, 0.15), p=0.3),  # Scale the frame randomly with a limit of -20% to +20%\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=20, p=0.3),  # Shift, scale, and rotate with limits\n",
    "        A.CenterCrop(height=171, width=175, p=4)\n",
    "    ])\n",
    "\n",
    "    # Augment each frame and add to augmented_frames\n",
    "    for frame in frames_list:\n",
    "        # Add original frame\n",
    "        augmented_frames.append(frame)\n",
    "        # Augment the frame multiple times\n",
    "        for _ in range(num_augmented_per_frame):\n",
    "            augmented_frame = augment_frame(frame,augmentation_pipeline)\n",
    "            augmented_frames.append(augmented_frame)\n",
    "\n",
    "    return augmented_frames\n",
    "\n",
    "# Example usage of augment_frames function\n",
    "# augmented_frames = augment_frames(frames_list, num_augmented_per_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fight_augmented=[]\n",
    "i=0\n",
    "for frame_list in fight_resize:\n",
    "  fight_augmented.append(augment_frames(frame_list,4))\n",
    "  # print('augmented {}th list'.format(i))\n",
    "  i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_image_matplotlib(frame_array):\n",
    "    frame_array_rgb = cv2.cvtColor(frame_array, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(frame_array_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_matplotlib(fight_augmented[3][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_fight_augmented=[]\n",
    "i=0\n",
    "for frame_list in non_fight_resize:\n",
    "  non_fight_augmented.append(augment_frames(frame_list,4))\n",
    "  # print('augmented {}th list'.format(i))\n",
    "  i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESSING DATA FOR VGG INPUTS AND TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "combined_list = fight_augmented + non_fight_augmented\n",
    "\n",
    "# fight_augmented gets label 1 and non_fight_augmented gets label 0\n",
    "labels = [1] * len(fight_augmented) + [0] * len(non_fight_augmented)\n",
    "\n",
    "combined_with_labels = list(zip(combined_list, labels))\n",
    "random.shuffle(combined_with_labels)\n",
    "\n",
    "shuffled_combined_list, shuffled_labels = zip(*combined_with_labels) #unzip\n",
    "\n",
    "# Convert back to lists\n",
    "input_data = list(shuffled_combined_list)\n",
    "input_labels = list(shuffled_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = 25\n",
    "\n",
    "#split the data\n",
    "test_df = input_data[:num_test_samples]   #frame sampling\n",
    "train_df = input_data[num_test_samples:]\n",
    "\n",
    "#split the labels\n",
    "test_labels = input_labels[:num_test_samples]\n",
    "train_labels = input_labels[num_test_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train-1 ',train_labels.count(1))\n",
    "print('train-0 ',train_labels.count(0))\n",
    "print('test-1 ',test_labels.count(1))\n",
    "print('test-0 ',test_labels.count(0))\n",
    "#no oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Bidirectional, Dropout, GlobalMaxPooling2D, GlobalMaxPooling1D, Masking, Attention, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "num_frames_per_video = 50  # Each video has 50 frames\n",
    "num_augmentations_per_frame = 4  # Assuming 4 augmentations per frame\n",
    "frame_height = 224\n",
    "frame_width = 224\n",
    "channels = 3  # Assuming RGB channels\n",
    "num_classes = 2  # Number of output classes (fight and non-fight)\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vgg16_feature_extractor(input_shape=(224, 224, 3)):\n",
    "    vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    vgg16.trainable = False\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = vgg16(inputs)\n",
    "    outputs = GlobalMaxPooling2D()(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_frames_training(video, model):\n",
    "  video_features = []\n",
    "  num_frames_per_segment = 5   #1 original + 4 augmented frames\n",
    "  num_segments = len(video) // num_frames_per_segment\n",
    "\n",
    "  for i in range(num_segments):\n",
    "      segment_features = []\n",
    "      for j in range(num_frames_per_segment):\n",
    "          frame_index = i * num_frames_per_segment + j\n",
    "          frame = np.expand_dims(video[frame_index], axis=0)\n",
    "          features = model.predict(frame)\n",
    "          segment_features.append(features)\n",
    "\n",
    "      # concatenated_features = np.concatenate(segment_features, axis=-1)\n",
    "      # video_features.append(concatenated_features)\n",
    "      averaged_features = np.mean(segment_features, axis=0)\n",
    "      video_features.append(averaged_features)\n",
    "\n",
    "  return video_features  #shape=(10,len of features of 5 frames concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACTING FEATURES FOR EVERY VIDEO\n",
    "def df_feature_extractor(input_data):\n",
    "  vgg16_feature_extractor = build_vgg16_feature_extractor()\n",
    "  extracted_features = []  #shape=(total videos, 10, features per frame ater 5 concatenate)\n",
    "  for video in input_data:  #inputs shape(no. of videos, frames per video, heigth, width, channels)\n",
    "    features = extract_features_from_frames_training(video, vgg16_feature_extractor)\n",
    "    extracted_features.append(features)\n",
    "  return extracted_features\n",
    "# train_data = df_feature_extractor(train_df)\n",
    "# print('train data features extracted')\n",
    "# test_data = df_feature_extractor(test_df)\n",
    "# print('test data features extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=df_feature_extractor(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=df_feature_extractor(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_np=np.array(train_data) #shape=(227,10,1,512)\n",
    "train_data_np=np.squeeze(train_data_np,axis=2) #shape=(227,10,512)\n",
    "\n",
    "test_data_np=np.array(test_data) #shape=(227,10,1,512)\n",
    "test_data_np=np.squeeze(test_data_np,axis=2) #shape=(227,10,512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # x = Masking(mask_value=0.0)(inputs)\n",
    "    x = Bidirectional(LSTM(256, return_sequences=True))(inputs)\n",
    "    x = Dropout(0.5)(x)\n",
    "    attention = Attention()([x, x])\n",
    "    x = Dense(1024, activation='relu')(attention)\n",
    "    x = Dropout(0.5)(x)\n",
    "    # x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(50, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training():\n",
    "  filepath = \"/content/model_weights\"\n",
    "  checkpoint = ModelCheckpoint(\n",
    "      filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "  )\n",
    "\n",
    "  train_labels_array = np.array(train_labels)\n",
    "  test_labels_array = np.array(test_labels)\n",
    "\n",
    "  seq_model = build_lstm_model((10,512))    #10->timestamps,,,,512->features of 1 timestamp(averaged out of 5 frames)\n",
    "  history = seq_model.fit(\n",
    "      train_data_np,\n",
    "      train_labels_array,\n",
    "      validation_split=0.2,\n",
    "      epochs=30,\n",
    "      batch_size=11,\n",
    "      callbacks=[checkpoint],\n",
    "  )\n",
    "\n",
    "  seq_model.load_weights(filepath)\n",
    "  _, accuracy = seq_model.evaluate(test_data_np, test_labels_array)\n",
    "  print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "  return history, seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_exp=model_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
